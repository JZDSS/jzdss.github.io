<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hexo 安装</title>
      <link href="2020/10/16/Hexo/"/>
      <url>2020/10/16/Hexo/</url>
      
        <content type="html"><![CDATA[<ul><li>基础功能：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry https:&#x2F;&#x2F;registry.npm.taobao.org</span><br><span class="line">npm install hexo-cli -g</span><br><span class="line">npm install hexo-server --save</span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></li></ul><hr><ul><li>主题：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;tufu9441&#x2F;maupassant-hexo.git themes&#x2F;maupassant</span><br><span class="line">npm install hexo-renderer-pug --save</span><br><span class="line">npm install hexo-renderer-sass --save</span><br></pre></td></tr></table></figure></li></ul><hr><ul><li>页面显示：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-archive --save</span><br><span class="line">npm install hexo-generator-category --save</span><br><span class="line">npm install hexo-generator-tag --save</span><br><span class="line">npm install hexo-generator-search --save</span><br><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CARAFE</title>
      <link href="2019/10/27/CARAFE/"/>
      <url>2019/10/27/CARAFE/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CARAFE_Content-Aware_ReAssembly_of_FEatures_ICCV_2019_paper.pdf">CARAFE: Content-Aware ReAssembly of FEatures</a>》</li></ul><hr><ul><li><p>目的</p><ul><li>解决检测、分割等任务中的上采样问题</li></ul></li></ul><hr><ul><li><p>特性</p><ul><li>大感受野</li><li>内容感知</li><li>轻量级</li></ul></li></ul><hr><ul><li><p>方案</p><p><img src="CARAFE.png"></p></li></ul><hr><ul><li><p>代码</p><ul><li><a href="https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetection</a></li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Object Detection </tag>
            
            <tag> light-weight </tag>
            
            <tag> upsample </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DAFS</title>
      <link href="2019/10/27/DAFS/"/>
      <url>2019/10/27/DAFS/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Dynamic_Anchor_Feature_Selection_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf">Dynamic Anchor Feature Selection for Single-Shot Object Detection</a>》</li></ul><hr><ul><li><p>问题</p><ul><li>RefineDet的ODM预测时规则卷积的感受野和refine过的anchor位置不匹配</li></ul></li></ul><hr><ul><li>方案<ul><li>提出Dynamic Anchor Feature Selection(DAFS)，根据refine后的anchor动态地选择特征</li><li>改进RefineDet的TCB模块，双向聚合特征谱</li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ClusDet</title>
      <link href="2019/10/27/ClusDet/"/>
      <url>2019/10/27/ClusDet/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Clustered_Object_Detection_in_Aerial_Images_ICCV_2019_paper.pdf">Clustered Object Detection in Aerial Images</a>》</li></ul><hr><ul><li><p>问题</p><ul><li><p>输入分辨率高</p></li><li><p>目标小</p></li><li><p>目标稀疏且非均匀分布</p></li></ul></li></ul><hr><ul><li><p>方案</p><ul><li>检测目标聚集区域，并从该区域内检测，将结果与全局检测结合</li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Object Detection </tag>
            
            <tag> aerial </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LRFNet</title>
      <link href="2019/10/27/LRFNet/"/>
      <url>2019/10/27/LRFNet/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Rich_Features_at_High-Speed_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf">Learning Rich Features at High-Speed for Single-Shot Object Detection</a>》</li></ul><hr><ul><li>问题<ul><li>现有多数目标检测网络是从ImageNet预训练的分类网络微调的，但是检测和分类任务对平移的敏感性要求不同，预训练网络和检测任务之间有一定差异，用分类预训练网络不是最优的。但是从头开始训练检测网络需要大量时间。</li></ul></li></ul><hr><ul><li>贡献<ul><li>用分类预训练网络做主体，外接一路从头训练的小网络，结合两者特征</li></ul></li></ul><p><img src="LRF.png"></p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Object Detection </tag>
            
            <tag> scratch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RPNN</title>
      <link href="2019/10/27/RPNN/"/>
      <url>2019/10/27/RPNN/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Relation_Parsing_Neural_Network_for_Human-Object_Interaction_Detection_ICCV_2019_paper.pdf">Relation Parsing Neural Network for Human-Object Interaction Detection</a>》</li></ul><hr><ul><li><p>问题</p><ul><li>目前的HOI方法从人和物体的特征以及他们的相对位置来预测人物交互，但是人实际是通过身体部件和物体进行交互的，身体部件应该得到更多的关注。</li></ul></li></ul><hr><ul><li><p>方案</p><ul><li><p>引入了身体部件特征</p></li><li><p>引入了(人-身体部件)注意力和(物体-身体部件)注意力</p></li></ul><p><img src="RPNN.png"></p></li></ul><hr><ul><li><p>相关</p><ul><li>《<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Haoshu_Fang_Pairwise_Body-Part_Attention_ECCV_2018_paper.pdf">Pairwise body-part attention for recognizing human-object interactions</a>》</li><li>《<a href="http://in.arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a>》</li><li>《<a href="https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf">Convolutional networks on graphs for learning molecular fingerprints</a>》</li><li>《<a href="https://papers.nips.cc/paper/6212-diffusion-convolutional-neural-networks.pdf">Diffusion-convolutional neural networks</a>》</li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Visual Cognition </tag>
            
            <tag> relationship </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ThunderNet</title>
      <link href="2019/10/27/ThunderNet/"/>
      <url>2019/10/27/ThunderNet/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qin_ThunderNet_Towards_Real-Time_Generic_Object_Detection_on_Mobile_Devices_ICCV_2019_paper.pdf">ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices</a>》</li></ul><hr><ul><li><p>问题</p><ul><li>目前没有轻量级二阶段检测模型</li></ul></li></ul><hr><ul><li><p>方案</p><ul><li><p>backbone</p><ul><li>着眼运行效率和感受野，以ShuffleNet v2为基础进行修改，将3x3的DW Conv全部改成5x5的DW Conv</li></ul></li><li><p>detection head</p><ul><li><p>设计Context Enhancement Module以增大感受野</p><p><img src="CEM.jpg" alt="Fig. 1 CEM"></p></li><li><p>设计Spacial Attention Module改善用于PSRoI align的特征</p><p><img src="SAM.jpg" alt="Fig. 2 SAM"></p></li><li><p>用5x5DWConv+1x1Conv取代3x3Conv</p></li></ul></li></ul></li></ul><p><img src="ThunderNet.jpg" alt="Fig. 3 网络结构"></p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Object Detection </tag>
            
            <tag> light-weight </tag>
            
            <tag> two-stage </tag>
            
            <tag> attention </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TridentNet</title>
      <link href="2019/10/27/TridentNet/"/>
      <url>2019/10/27/TridentNet/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Scale-Aware_Trident_Networks_for_Object_Detection_ICCV_2019_paper.pdf">Scale-Aware Trident Networks for Object Detection</a>》</li></ul><hr><ul><li><p>贡献</p><ul><li><p>通过对backbone网络的部分层设置不同的扩张率，探索了感受野对目标检测效果的影响</p><ul><li><p>感受野需要和目标尺度相匹配</p></li><li><p>有效感受野小于理论感受野</p></li><li><p>感受野需要在大目标和小目标之间找到平衡</p></li></ul></li><li><p>权重共享，扩张率不同的多条分支生成不同感受野的特征以应对不同尺度的目标</p><p><img src="TridentNet.png"></p></li><li><p>类似SNIP，采用scale-aware的训练方法</p></li><li><p>可以使用主分支检测达到近似三分支的精度(扩展尺度范围为0到∞相当于放弃了scale-aware训练策略？)</p></li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Object Detection </tag>
            
            <tag> scale </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VrR-VG</title>
      <link href="2019/10/27/VrR-VG/"/>
      <url>2019/10/27/VrR-VG/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.pdf">VrR-VG: Refocusing Visually-Relevant Relationships</a>》</li></ul><hr><ul><li><p>问题</p><ul><li>现有的关系预测模型会拟合静态偏差（空间位置、固定搭配等可以从非视觉信息获取的结果）而不是学习视觉信息</li></ul></li></ul><hr><ul><li><p>原因</p><ul><li><p>神经网络有强大的从非视觉信息预测的能力</p></li><li><p>空间关系(on、in等)完全可以通过bbox坐标来预测</p></li><li><p>某些关系(wear、ride、has等)可以从语言先验或者统计分析来预测</p><p><img src="distribution2.png"></p></li><li><p>现有数据集有很多上述标签</p><p><img src="distribution1.png"></p></li></ul></li></ul><hr><ul><li><p>贡献</p><ul><li><p>新的关系数据集<a href="http://vrr-vg.com/">VrR-VG</a></p></li><li><p>关系感知模型</p><ul><li><p>可以通过坐标、类别等信息来预测的关系是视觉无关的</p></li><li><p>Visual Discriminator: VD-Net通过坐标、类别、词汇等非视觉信息来预测关系</p></li><li><p>用VD-Net无法准确预测的关系标签来训练视觉感知模型并用于VQA等任务</p></li></ul></li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ICCV </tag>
            
            <tag> Visual Cognition </tag>
            
            <tag> relationship </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FishNet</title>
      <link href="2018/12/02/FishNet/"/>
      <url>2018/12/02/FishNet/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf">FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction</a>》</li></ul><hr><ul><li>问题<ul><li>目前区域/像素级别的任务都在使用分类预训练网络作为backbone，而没有一个专门设计的backbone</li><li>目前的工作无法直接地将梯度反传到浅层</li></ul></li></ul><hr><ul><li><p>方案</p><ul><li><p>提出了FishNet结构（降采样-上采样-降采样结构）</p><p><img src="FishNet.jpg" alt="FishNet"></p></li><li><p>对ResBlock进行修改，取消了identity mapping中的卷积，降采样/增加通道放在外部</p><p><img src="resblock.jpg" alt="ResBlock"></p></li></ul></li></ul><hr><ul><li>相关<ul><li>《Stacked Hourglass Networks for Human Pose Estimation》</li><li>《DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation》</li><li>《M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network》</li><li>《CBNet: A Novel Composite Backbone Network Architecture for Object Detection》</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fusion </tag>
            
            <tag> NeurIPS </tag>
            
            <tag> Object Detection </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Imbalanced Attribute Classification using Visual Attention Aggregation</title>
      <link href="2018/09/08/Deep%20Imbalanced%20Attribute%20Classification%20using%20Visual%20Attention%20Aggregation/"/>
      <url>2018/09/08/Deep%20Imbalanced%20Attribute%20Classification%20using%20Visual%20Attention%20Aggregation/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Sarafianos_Deep_Imbalanced_Attribute_ECCV_2018_paper.pdf">Deep Imbalanced Attribute Classification using Visual Attention Aggregation</a>》</li></ul><hr><ul><li>问题<ul><li>现有方法忽略了空间和语义信息</li><li>属性分类中类别不均衡</li></ul></li></ul><hr><ul><li>方案<ul><li>每种属性分别引入了空间注意力机制</li><li>属性频率指数加权的focal loss</li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ECCV </tag>
            
            <tag> loss </tag>
            
            <tag> classification </tag>
            
            <tag> attribute </tag>
            
            <tag> attention </tag>
            
            <tag> Visual Cognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DetNet</title>
      <link href="2018/09/08/DetNet/"/>
      <url>2018/09/08/DetNet/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Zeming_Li_DetNet_Design_Backbone_ECCV_2018_paper.pdf">DetNet: Design backbone for object detection</a>》</li></ul><hr><ul><li>问题<ul><li>分类网络的设计准则（大幅降采样）不适合检测任务（定位）</li><li>RPN等网络虽然进行了优化，但仍有一些问题<ul><li>backbone的阶段（stage）数和检测的阶段数不同</li><li>由于深层特征谱的大步长，大目标定位不准确</li><li>小目标在降采样过程中消失</li></ul></li></ul></li></ul><hr><ul><li><p>方案</p><ul><li>设计新的backbone<ul><li>stage数和检测相同</li><li>深层stage保持较高的分辨率</li><li>通过扩张残差块增大感受野</li></ul></li></ul><p><img src="overview.png" alt="overview"><br><img src="detail.png" alt="detail"></p></li></ul><hr><ul><li>缺陷<ul><li>仍需ImageNet预训练</li></ul></li></ul><hr><ul><li><p>代码</p><p><a href="https://github.com/zengarden/DetNet">https://github.com/zengarden/DetNet</a></p></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ECCV </tag>
            
            <tag> Object Detection </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparseNet</title>
      <link href="2018/09/08/SparseNet/"/>
      <url>2018/09/08/SparseNet/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><ul><li>论文：《<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Ligeng_Zhu_Sparsely_Aggregated_Convolutional_ECCV_2018_paper.pdf">Sparsely Aggregated Convolutional Networks</a>》</li></ul><hr><ul><li>问题：<ul><li>ResNet型密集聚合（+）：聚合后的特征不可分解，深层特征稀释或破坏浅层特征</li><li>DenseNet型密集聚合（concat）：参数以\(O(N^2)\)增加，消耗大量参数和运算来处理同一块特征</li></ul></li></ul><hr><ul><li>方案：<ul><li>保留短梯度路径（skip connection）的优势，但避免类ResNet、DenseNet的稠密聚合</li><li>\(y_l=F_l(\bigotimes(y_{l-c^0},y_{l-c^1},y_{l-c^2}…y_{l-c^k}))\)</li></ul></li></ul><hr><p><img src="SparseNet.png"></p><hr><ul><li>相关：<ul><li><a href="">PRN</a></li><li><a href="">CSPNet</a></li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ECCV </tag>
            
            <tag> light-weight </tag>
            
            <tag> backbone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cascade R-CNN</title>
      <link href="2018/06/18/Cascade-R-CNN/"/>
      <url>2018/06/18/Cascade-R-CNN/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf">Cascade R-CNN: Delving into High Quality Object Detection</a>》</li></ul><hr><ul><li><p>问题</p><p>用低IoU阈值训练的检测器往往会给出带噪的检测结果，但是一味地提升训练时的IoU阈值也会使检测器性能下降。</p></li></ul><hr><ul><li><p>原因</p><ul><li>随着IoU阈值提高，训练时的正样本数量急剧下降</li><li>训练时正样本与gt的IoU较高，推理时候选样本和gt的IoU可能没那么高，两者不匹配</li></ul></li></ul><hr><ul><li><p>方案</p><ul><li>级联回归，每一级一次上调IoU阈值并调整回归误差的均值方差。</li></ul></li></ul><hr><ul><li>相关<ul><li>《Object detection with discriminatively trained part-based models》</li><li>《Object detection via a multiregion and semantic segmentation-aware CNN model》</li></ul></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CVPR </tag>
            
            <tag> Object Detection </tag>
            
            <tag> two-stage </tag>
            
            <tag> cascade </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MS-CNN</title>
      <link href="2016/10/10/MS-CNN/"/>
      <url>2016/10/10/MS-CNN/</url>
      
        <content type="html"><![CDATA[<ul><li>论文：《<a href="https://arxiv.org/pdf/1607.07155.pdf">A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</a>》</li></ul><hr><table><thead><tr><th align="center">问题</th><th align="center">方案</th></tr></thead><tbody><tr><td align="center">感受野和目标尺度不匹配问题</td><td align="center">多尺度生成proposal，每个特征谱预测特定尺度的目标</td></tr><tr><td align="center">对浅层的BP容易带来训练不稳定</td><td align="center">在conv4_3后接一个buffer conv层</td></tr><tr><td align="center"></td><td align="center">与gt的IoU小于0.2的proposal定为负样本</td></tr><tr><td align="center">深层特征对小目标不敏感</td><td align="center">conv4_3的输出上采样2x后作为RoI Pooling的输入</td></tr><tr><td align="center">上下文有利于分割和检测</td><td align="center">RoI的特征和其上下文同时做RoI Pooling并concat</td></tr></tbody></table><p><img src="RPN.png" alt="Probosal sub-network"></p><p><img src="DET.png" alt="Object detection sub-network"></p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ECCV </tag>
            
            <tag> Object Detection </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
